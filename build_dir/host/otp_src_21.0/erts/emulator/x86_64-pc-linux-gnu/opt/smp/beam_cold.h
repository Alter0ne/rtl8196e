/*
 *  Warning: Do not edit this file.
 *  Auto-generated by 'beam_makeops'.
 */

{
  Eterm nif_bif_result;
  Eterm bif_nif_arity;
  BifFunction vbf;
  ErlHeapFragment *live_hf_end;
  ErtsCodeMFA *codemfa;
  OpCase(apply_bif):
  {
    /*
    * At this point, I points to the code[0] in the export entry for
    * the BIF:
    *
    * code[-3]: Module
    * code[-2]: Function
    * code[-1]: Arity
    * code[0]: &&apply_bif
    * code[1]: Function pointer to BIF function
    */

    if (!((FCALLS - 1) > 0 || (FCALLS - 1) > neg_o_reds)) {
      /* If we have run out of reductions, we do a context
      switch before calling the bif */
      goto context_switch;
    }

    codemfa = erts_code_to_codemfa(I);

    ERTS_MSACC_SET_BIF_STATE_CACHED_X(codemfa->module, (BifFunction)Arg(0));


    /* In case we apply process_info/1,2 or load_nif/1 */
    c_p->current = codemfa;
    c_p->i = I;
    ASSERT(VALID_INSTR(*c_p->i));;     /* In case we apply check_process_code/2. */
    c_p->arity = 0;       /* To allow garbage collection on ourselves
    * (check_process_code/2).
    */
    DTRACE_BIF_ENTRY(c_p, codemfa);

    SWAPOUT;
    ERTS_DBG_CHK_REDS(c_p, FCALLS - 1);
    c_p->fcalls = FCALLS - 1;
    vbf = (BifFunction) Arg(0);
    PROCESS_MAIN_CHK_LOCKS(c_p);
    bif_nif_arity = codemfa->arity;
    ASSERT(bif_nif_arity <= 4);
    ERTS_UNREQ_PROC_MAIN_LOCK(c_p);
    ERTS_VERIFY_UNUSED_TEMP_ALLOC(c_p);
    {
      ErtsBifFunc bf = vbf;
      ASSERT(!ERTS_PROC_IS_EXITING(c_p));
      live_hf_end = c_p->mbuf;
      ERTS_CHK_MBUF_SZ(c_p);
      nif_bif_result = (*bf)(c_p, reg, I);
      ERTS_CHK_MBUF_SZ(c_p);
      ASSERT(!ERTS_PROC_IS_EXITING(c_p) ||
      is_non_value(nif_bif_result));
      ERTS_VERIFY_UNUSED_TEMP_ALLOC(c_p);
      PROCESS_MAIN_CHK_LOCKS(c_p);
    }
    /* We have to update the cache if we are enabled in order
    to make sure no book keeping is done after we disabled
    msacc. We don't always do this as it is quite expensive. */
    if (ERTS_MSACC_IS_ENABLED_CACHED_X())
    ERTS_MSACC_UPDATE_CACHE_X();
    ERTS_MSACC_SET_STATE_CACHED_M_X(ERTS_MSACC_STATE_EMULATOR);
    DTRACE_BIF_RETURN(c_p, codemfa);
  }
  goto nif_bif__epilogue;

  OpCase(call_nif):
  {
    /*
    * call_nif is always first instruction in function:
    *
    * I[-3]: Module
    * I[-2]: Function
    * I[-1]: Arity
    * I[0]: &&call_nif
    * I[1]: Function pointer to NIF function
    * I[2]: Pointer to erl_module_nif
    * I[3]: Function pointer to dirty NIF
    *
    * This layout is determined by the NifExport struct
    */

    ERTS_MSACC_SET_STATE_CACHED_M_X(ERTS_MSACC_STATE_NIF);

    codemfa = erts_code_to_codemfa(I);

    c_p->current = codemfa; /* current and vbf set to please handle_error */

    DTRACE_NIF_ENTRY(c_p, codemfa);

    HEAVY_SWAPOUT;

    PROCESS_MAIN_CHK_LOCKS(c_p);
    bif_nif_arity = codemfa->arity;
    ERTS_UNREQ_PROC_MAIN_LOCK(c_p);

    ASSERT(!ERTS_PROC_IS_EXITING(c_p));
    {
      typedef Eterm NifF(struct enif_environment_t*, int argc, Eterm argv[]);
      NifF* fp = vbf = (NifF*) I[1];
      struct enif_environment_t env;
      ASSERT(c_p->scheduler_data);
      live_hf_end = c_p->mbuf;
      ERTS_CHK_MBUF_SZ(c_p);
      erts_pre_nif(&env, c_p, (struct erl_module_nif*)I[2], NULL);

      ASSERT((c_p->scheduler_data)->current_nif == NULL);
      (c_p->scheduler_data)->current_nif = &env;

      nif_bif_result = (*fp)(&env, bif_nif_arity, reg);
      if (env.exception_thrown)
      nif_bif_result = THE_NON_VALUE;

      ASSERT((c_p->scheduler_data)->current_nif == &env);
      (c_p->scheduler_data)->current_nif = NULL;

      erts_post_nif(&env);
      ERTS_CHK_MBUF_SZ(c_p);

      PROCESS_MAIN_CHK_LOCKS(c_p);
      ERTS_VERIFY_UNUSED_TEMP_ALLOC(c_p);
      ERTS_MSACC_SET_STATE_CACHED_M_X(ERTS_MSACC_STATE_EMULATOR);
      ASSERT(!env.exiting);
      ASSERT(!ERTS_PROC_IS_EXITING(c_p));
    }

    DTRACE_NIF_RETURN(c_p, codemfa);
  }
  goto nif_bif__epilogue;

  nif_bif__epilogue:
  {
    BeamInstr next_pf = BeamCodeAddr(I[1]);
    ERTS_REQ_PROC_MAIN_LOCK(c_p);
    ERTS_HOLE_CHECK(c_p);
    if (ERTS_IS_GC_DESIRED(c_p)) {
      nif_bif_result = erts_gc_after_bif_call_lhf(c_p, live_hf_end,
      nif_bif_result,
      reg, bif_nif_arity);
    }
    SWAPIN;  /* There might have been a garbage collection. */
    FCALLS = c_p->fcalls;
    ERTS_DBG_CHK_REDS(c_p, FCALLS);
    if (ERTS_LIKELY(is_value(nif_bif_result))) {
      r(0) = nif_bif_result;
      CHECK_TERM(r(0));
      SET_I(c_p->cp);
      c_p->cp = 0;
      Goto(*I);
    } else if (c_p->freason == TRAP) {
      SET_I(c_p->i);
      if (c_p->flags & F_HIBERNATE_SCHED) {
        c_p->flags &= ~F_HIBERNATE_SCHED;
        goto do_schedule;
      }
      Dispatch();
    }
    I = handle_error(c_p, c_p->cp, reg, c_p->current);
    goto post_error_handling;
    I += 1;
    ASSERT(VALID_INSTR(next_pf));
    GotoPF(next_pf);
  }

}

OpCase(badarg_j):
{
  c_p->freason = BADARG;

  /*
  * In a correctly working program, we expect failures in
  * guards to be more likely than failures in bodies.
  */

  if (ERTS_LIKELY(I[1])) {
    ASSERT(VALID_INSTR(*(I + (I[1]) + 0)));
    I += I[1] + 0;;
    Goto(*I);;
  }
  goto find_func_info;;;

}

OpCase(badmatch_x):
{
  c_p->fvalue = xb(I[1]);
  c_p->freason = BADMATCH;
  goto find_func_info;

}

OpCase(case_end_x):
{
  c_p->fvalue = xb(I[1]);
  c_p->freason = EXC_CASE_CLAUSE;
  goto find_func_info;

}

OpCase(deallocate_Q):
{
  SET_CP(c_p, (BeamInstr *) cp_val(*E));
  E = ADD_BYTE_OFFSET(E, I[1]);
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(ensure_map_x):
{
  if (is_not_map(xb(I[1]))) {
    c_p->freason = BADMAP;
    c_p->fvalue = xb(I[1]);
    goto find_func_info;;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(i_debug_breakpoint):
{
  HEAVY_SWAPOUT;
  I = call_error_handler(c_p, erts_code_to_codemfa(I), reg, am_breakpoint);
  HEAVY_SWAPIN;
  if (I) {
    Goto(*I);
  }
  goto handle_error;

}

OpCase(i_generic_breakpoint):
{
  BeamInstr real_I;
  HEAVY_SWAPOUT;
  real_I = erts_generic_breakpoint(c_p, erts_code_to_codeinfo(I), reg);
  HEAVY_SWAPIN;
  ASSERT(VALID_INSTR(real_I));
  Goto(real_I);

}

OpCase(i_get_tuple_element_xPy):
{
  Eterm tmp_packed1 = I[1];
  BeamInstr next_pf = BeamCodeAddr(I[3]);
  Eterm* src = ADD_BYTE_OFFSET(tuple_val(xb(tmp_packed1&BEAM_LOOSE_MASK)), I[2]);
  yb((tmp_packed1>>BEAM_LOOSE_SHIFT)) = *src;
  I += 3;
  ASSERT(VALID_INSTR(next_pf));
  GotoPF(next_pf);
}

OpCase(i_get_tuple_element_yPy):
{
  Eterm tmp_packed1 = I[1];
  BeamInstr next_pf = BeamCodeAddr(I[3]);
  Eterm* src = ADD_BYTE_OFFSET(tuple_val(yb(tmp_packed1&BEAM_LOOSE_MASK)), I[2]);
  yb((tmp_packed1>>BEAM_LOOSE_SHIFT)) = *src;
  I += 3;
  ASSERT(VALID_INSTR(next_pf));
  GotoPF(next_pf);
}

OpCase(i_hibernate):
{
  HEAVY_SWAPOUT;
  if (erts_hibernate(c_p, reg)) {
    FCALLS = c_p->fcalls;
    c_p->flags &= ~F_HIBERNATE_SCHED;
    goto do_schedule;
  } else {
    HEAVY_SWAPIN;
    I = handle_error(c_p, I, reg, &bif_export[BIF_hibernate_3]->info.mfa);
    goto post_error_handling;
  }

}

OpCase(i_make_fun_Wt):
{
  BeamInstr next_pf = BeamCodeAddr(I[3]);
  HEAVY_SWAPOUT;
  x(0) = new_fun(c_p, reg, (ErlFunEntry *) I[1], I[2]);
  HEAVY_SWAPIN;
  I += 3;
  ASSERT(VALID_INSTR(next_pf));
  GotoPF(next_pf);
}

OpCase(i_perf_counter):
{
  BeamInstr next_pf = BeamCodeAddr(I[1]);
  ErtsSysPerfCounter ts;

  ts = erts_sys_perf_counter();
  if (IS_SSMALL(ts)) {
    r(0) = make_small((Sint)ts);
  } else {
    do {
      Uint need = ERTS_SINT64_HEAP_SIZE(ts) + 0;
      if (ERTS_UNLIKELY(E - HTOP < need)) {
        SWAPOUT;
        PROCESS_MAIN_CHK_LOCKS(c_p);
        FCALLS -= erts_garbage_collect_nobump(c_p, need, reg, 0, FCALLS);
        ERTS_VERIFY_UNUSED_TEMP_ALLOC(c_p);
        PROCESS_MAIN_CHK_LOCKS(c_p);
        SWAPIN;
      }
      HEAP_SPACE_VERIFIED(ERTS_SINT64_HEAP_SIZE(ts));
    } while (0);
    r(0) = make_big(HTOP);
#if defined(ARCH_32)
    if (ts >= (((Uint64) 1) << 32)) {
      *HTOP = make_pos_bignum_header(2);
      BIG_DIGIT(HTOP, 0) = (Uint) (ts & ((Uint) 0xffffffff));
      BIG_DIGIT(HTOP, 1) = (Uint) ((ts >> 32) & ((Uint) 0xffffffff));
      HTOP += 3;
    }
    else
#endif
    {
      *HTOP = make_pos_bignum_header(1);
      BIG_DIGIT(HTOP, 0) = (Uint) ts;
      HTOP += 2;
    }
  }
  I += 1;
  ASSERT(VALID_INSTR(next_pf));
  GotoPF(next_pf);
}

OpCase(i_raise):
{
  Eterm raise_trace = x(2);
  Eterm raise_value = x(1);
  struct StackTrace *s;

  c_p->fvalue = raise_value;
  c_p->ftrace = raise_trace;
  s = get_trace_from_exc(raise_trace);
  if (s == NULL) {
    c_p->freason = EXC_ERROR;
  } else {
    c_p->freason = PRIMARY_EXCEPTION(s->freason);
  }
  goto find_func_info;

}

OpCase(i_return_time_trace):
{
  BeamInstr *pc = (BeamInstr *) (UWord) E[0];
  SWAPOUT;
  erts_trace_time_return(c_p, erts_code_to_codeinfo(pc));
  SWAPIN;
  c_p->cp = NULL;
  SET_I((BeamInstr *) cp_val(E[1]));
  E += 2;
  Goto(*I);

}

OpCase(i_return_to_trace):
{
  if (IS_TRACED_FL(c_p, F_TRACE_RETURN_TO)) {
    Uint *cpp = (Uint*) E;
    for(;;) {
      ASSERT(is_CP(*cpp));
      if (IsOpCode(*cp_val(*cpp), return_trace)) {
        do
        ++cpp;
        while (is_not_CP(*cpp));
        cpp += 2;
      } else if (IsOpCode(*cp_val(*cpp), i_return_to_trace)) {
        do
        ++cpp;
        while (is_not_CP(*cpp));
      } else {
        break;
      }
    }
    SWAPOUT;		/* Needed for shared heap */
    ERTS_UNREQ_PROC_MAIN_LOCK(c_p);
    erts_trace_return_to(c_p, cp_val(*cpp));
    ERTS_REQ_PROC_MAIN_LOCK(c_p);
    SWAPIN;
  }
  c_p->cp = NULL;
  SET_I((BeamInstr *) cp_val(E[0]));
  E += 1;
  Goto(*I);

}

OpCase(i_wait_error):
{
  c_p->freason = EXC_TIMEOUT_VALUE;
  goto find_func_info;

}

OpCase(i_wait_error_locked):
{
  erts_proc_unlock(c_p, ERTS_PROC_LOCKS_MSG_RECEIVE);
  c_p->freason = EXC_TIMEOUT_VALUE;
  goto find_func_info;

}

OpCase(i_yield):
{
  /* This is safe as long as REDS_IN(c_p) is never stored
  * in c_p->arg_reg[0]. It is currently stored in c_p->def_arg_reg[5].
  */
  c_p->arg_reg[0] = am_true;
  c_p->arity = 1; /* One living register (the 'true' return value) */
  SWAPOUT;
  c_p->i = I+1;
  ASSERT(VALID_INSTR(*c_p->i));;
  c_p->current = NULL;
  goto do_schedule;

}

OpCase(if_end):
{
  c_p->freason = EXC_IF_CLAUSE;
  goto find_func_info;

}

OpCase(is_atom_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_atom(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_binary_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_binary(yb((tmp_packed1>>BEAM_WIDE_SHIFT))) || binary_bitsize(yb((tmp_packed1>>BEAM_WIDE_SHIFT))) != 0) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_bitstring_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_binary(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_boolean_fx):
{
  Eterm tmp_packed1 = I[1];
  if ((xb((tmp_packed1>>BEAM_WIDE_SHIFT))) != am_true && (xb((tmp_packed1>>BEAM_WIDE_SHIFT))) != am_false) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_boolean_fy):
{
  Eterm tmp_packed1 = I[1];
  if ((yb((tmp_packed1>>BEAM_WIDE_SHIFT))) != am_true && (yb((tmp_packed1>>BEAM_WIDE_SHIFT))) != am_false) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_float_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_float(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_ge_fss):
{
  Eterm targ1;
  Eterm targ2;
  GetR(1, targ1);
  GetR(2, targ2);
  {
    CMP_GE_ACTION(targ1, targ2, ASSERT(VALID_INSTR(*(I + (I[1]) + 0)));
    I += I[1] + 0;;
    Goto(*I););
    I += 4;
    ASSERT(VALID_INSTR(*I));
    Goto(*I);
  }
}

OpCase(is_list_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_list(yb((tmp_packed1>>BEAM_WIDE_SHIFT))) && is_not_nil(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_lt_fss):
{
  Eterm targ1;
  Eterm targ2;
  GetR(1, targ1);
  GetR(2, targ2);
  {
    CMP_LT_ACTION(targ1, targ2, ASSERT(VALID_INSTR(*(I + (I[1]) + 0)));
    I += I[1] + 0;;
    Goto(*I););
    I += 4;
    ASSERT(VALID_INSTR(*I));
    Goto(*I);
  }
}

OpCase(is_number_fx):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_integer(xb((tmp_packed1>>BEAM_WIDE_SHIFT))) && is_not_float(xb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_number_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_integer(yb((tmp_packed1>>BEAM_WIDE_SHIFT))) && is_not_float(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_pid_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_pid(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_port_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_port(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(is_reference_fy):
{
  Eterm tmp_packed1 = I[1];
  if (is_not_ref(yb((tmp_packed1>>BEAM_WIDE_SHIFT)))) {
    ASSERT(VALID_INSTR(*(I + (fb(tmp_packed1&BEAM_WIDE_MASK)) + 0)));
    I += fb(tmp_packed1&BEAM_WIDE_MASK) + 0;;
    Goto(*I);;
  }
  I += 2;
  ASSERT(VALID_INSTR(*I));
  Goto(*I);
}

OpCase(node_y):
{
  BeamInstr next_pf = BeamCodeAddr(I[2]);
  yb(I[1]) = erts_this_node->sysname;
  I += 2;
  ASSERT(VALID_INSTR(next_pf));
  GotoPF(next_pf);
}

OpCase(put_list_ssd):
{
  Eterm targ1;
  Eterm targ2;
  Eterm dst = I[3];
  Eterm* dst_ptr = REG_TARGET_PTR(dst);
  BeamInstr next_pf = BeamCodeAddr(I[4]);
  GetR(0, targ1);
  GetR(1, targ2);
  {
    HTOP[0] = targ1;
    HTOP[1] = targ2;
    *dst_ptr = make_list(HTOP);
    HTOP += 2;
    I += 4;
    ASSERT(VALID_INSTR(next_pf));
    GotoPF(next_pf);
  }
}

OpCase(return_trace):
{
  ErtsCodeMFA* mfa = (ErtsCodeMFA *)(E[0]);

  SWAPOUT;		/* Needed for shared heap */
  ERTS_UNREQ_PROC_MAIN_LOCK(c_p);
  erts_trace_return(c_p, mfa, r(0), ERTS_TRACER_FROM_ETERM(E+1)/* tracer */);
  ERTS_REQ_PROC_MAIN_LOCK(c_p);
  SWAPIN;
  c_p->cp = NULL;
  SET_I((BeamInstr *) cp_val(E[2]));
  E += 3;
  Goto(*I);

}

OpCase(system_limit_j):
{
  c_p->freason = SYSTEM_LIMIT;

  /*
  * In a correctly working program, we expect failures in
  * guards to be more likely than failures in bodies.
  */

  if (ERTS_LIKELY(I[1])) {
    ASSERT(VALID_INSTR(*(I + (I[1]) + 0)));
    I += I[1] + 0;;
    Goto(*I);;
  }
  goto find_func_info;;;

}

OpCase(trace_jump_W):
{
  SET_I((BeamInstr *) I[1]);
  Goto(*I);
}

OpCase(try_case_end_s):
{
  Eterm targ1;
  GetR(0, targ1);
  {
    c_p->fvalue = targ1;
    c_p->freason = EXC_TRY_CLAUSE;
    goto find_func_info;

  }
}

